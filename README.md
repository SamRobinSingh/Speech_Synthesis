# ğŸ—£ï¸ Speech Synthesis AI Framework


## ğŸ“ Abstract

In the evolving landscape of Human-Computer Interaction (HCI), **natural voice-based communication** plays a pivotal role in bridging the gap between users and machines. This project presents a **conversational AI framework** that simulates human-like interactions using three critical modules: **Speech-to-Text (STT)**, **Text Generation**, and **Text-to-Speech (TTS)**.

The system begins by capturing the user's voice, accurately transcribes it into text using OpenAIâ€™s **Whisper** model, processes it through a **language model (LLaMA 3.2)** to generate a relevant, context-aware response, and finally synthesizes the response back into human-like speech using **Kokoro TTS**, an open-weight text-to-speech engine.

This complete pipeline forms a real-time, end-to-end **speech-in â†’ speech-out** solution that mimics human conversation, enabling its use in **virtual assistants**, **accessibility technologies**, **customer support**, and **educational applications**.

---

## ğŸ“Œ Project Overview

The project is a **Speech-based AI Assistant** that understands your **spoken questions**, processes them with AI, and gives you a **spoken answer**.

Itâ€™s like talking to a smart assistant â€” you speak, it listens, thinks, and speaks back â€” all using AI.

> ğŸ”„ It follows this pipeline:
**Speech Input (You talk) â†’ Text â†’ AI Response â†’ Speech Output**

This is achieved through:
1. **Speech-to-Text** using **Whisper**
2. **Text Generation** using **LLaMA or GPT-style model**
3. **Text-to-Speech** using **Kokoro TTS**

---

## ğŸ”¬ Methodology

### Step-by-Step Breakdown:

1. ğŸ™ï¸ **Speech Input**
   - The user speaks a query.
   - Audio is recorded as a `.wav` file.

2. ğŸ“ **Speech-to-Text (STT)**
   - The recorded audio is passed to **Whisper**, a deep learning model for speech recognition.
   - It transcribes spoken words into text with high accuracy, even in noisy environments.

3. ğŸ¤– **Text Generation**
   - The transcribed text (e.g., "What is AI?") is passed to a **Large Language Model** (like **LLaMA 3.2**).
   - The model generates a human-like text response based on the question.

4. ğŸ”Š **Text-to-Speech (TTS)**
   - The AI-generated response is fed into the **Kokoro TTS** model.
   - It synthesizes expressive, clear speech from the response text.
   - The final output is played back to the user.

---

## ğŸš€ Features

- ğŸ™ï¸ Real-time Speech Recognition with Whisper
- ğŸ§  Intelligent response generation using LLM (LLaMA 3.2)
- ğŸ”Š Natural speech synthesis using Kokoro
- ğŸ“ Modular design for easy extension and experimentation

---

## ğŸ› ï¸ Installation & Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/SamRobinSingh/Speech_Synthesis.git
   cd Speech_Synthesis
2. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
3. **Install Chocolatey (Windows only)**
   
   Follow instructions: https://chocolatey.org/install
   
6. **Install FFmpeg (required for Whisper)**
   ```bash
   choco install ffmpeg
7. **Run the main program**
   ```bash
   python main.py



